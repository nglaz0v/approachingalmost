{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3590a3f",
   "metadata": {},
   "source": [
    "# Approaching text classification/regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e079bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(sentence, pos, neg):\n",
    "    \"\"\"\n",
    "    This function returns sentiment of sentence\n",
    "    :param sentence: sentence, a string\n",
    "    :param pos: set of positive words\n",
    "    :param neg: set of negative words\n",
    "    :return: returns positive, negative or neutral sentiment\n",
    "    \"\"\"\n",
    "    # split sentence by a space\n",
    "    # \"this is a sentence!\" becomes:\n",
    "    # [\"this\", \"is\" \"a\", \"sentence!\"]\n",
    "    # note that im splitting on all whitespaces\n",
    "    # if you want to split by space use .split(\" \")\n",
    "    sentence = sentence.split()\n",
    "    \n",
    "    # make sentence into a set\n",
    "    sentence = set(sentence)\n",
    "    \n",
    "    # check number of common words with positive\n",
    "    num_common_pos = len(sentence.intersection(pos))\n",
    "    \n",
    "    # check number of common words with negative\n",
    "    num_common_neg = len(sentence.intersection(neg))\n",
    "    \n",
    "    # make conditions and return\n",
    "    # see how return used eliminates if else\n",
    "    if num_common_pos > num_common_neg:\n",
    "        return \"positive\"\n",
    "    if num_common_pos < num_common_neg:\n",
    "        return \"negative\"\n",
    "    return \"neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b94b0",
   "metadata": {},
   "source": [
    "## tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0451ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['hi,', 'how', 'are', 'you?'], ['hi', ',', 'how', 'are', 'you', '?'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"hi, how are you?\"\n",
    "\n",
    "sentence.split(), word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e969b9fb",
   "metadata": {},
   "source": [
    "## bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27dfcadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 22)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 22)\t2\n",
      "  (2, 0)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 22)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 20)\t1\n",
      "  (4, 21)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "    \"hello, how are you?\",\n",
    "    \"im getting bored at home. And you? What do you think?\",\n",
    "    \"did you know about counts\",\n",
    "    \"let's see if this works!\",\n",
    "    \"YES!!!!\"\n",
    "]\n",
    "\n",
    "# initialize CountVectorizer\n",
    "ctv = CountVectorizer()\n",
    "\n",
    "# fit the vectorizer on corpus\n",
    "ctv.fit(corpus)\n",
    "\n",
    "corpus_transformed = ctv.transform(corpus)\n",
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c005e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 9, 'how': 11, 'are': 2, 'you': 22, 'im': 13, 'getting': 8, 'bored': 4, 'at': 3, 'home': 10, 'and': 1, 'what': 19, 'do': 7, 'think': 17, 'did': 6, 'know': 14, 'about': 0, 'counts': 5, 'let': 15, 'see': 16, 'if': 12, 'this': 18, 'works': 20, 'yes': 21}\n"
     ]
    }
   ],
   "source": [
    "print(ctv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12adf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 14, ',': 2, 'how': 16, 'are': 7, 'you': 27, '?': 4, 'im': 18, 'getting': 13, 'bored': 9, 'at': 8, 'home': 15, '.': 3, 'and': 6, 'what': 24, 'do': 12, 'think': 22, 'did': 11, 'know': 19, 'about': 5, 'counts': 10, 'let': 20, \"'s\": 1, 'see': 21, 'if': 17, 'this': 23, 'works': 25, '!': 0, 'yes': 26}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "    \"hello, how are you?\",\n",
    "    \"im getting bored at home. And you? What do you think?\",\n",
    "    \"did you know about counts\",\n",
    "    \"let's see if this works!\",\n",
    "    \"YES!!!!\"\n",
    "]\n",
    "\n",
    "# initialize CountVectorizer with word_tokenize from nltk\n",
    "# as the tokenizer\n",
    "ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "# fit the vectorizer on corpus\n",
    "ctv.fit(corpus)\n",
    "\n",
    "corpus_transformed = ctv.transform(corpus)\n",
    "print(ctv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb8d5c",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8713ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 27)\t0.2965698850220162\n",
      "  (0, 16)\t0.4428321995085722\n",
      "  (0, 14)\t0.4428321995085722\n",
      "  (0, 7)\t0.4428321995085722\n",
      "  (0, 4)\t0.35727423026525224\n",
      "  (0, 2)\t0.4428321995085722\n",
      "  (1, 27)\t0.35299699146792735\n",
      "  (1, 24)\t0.2635440111190765\n",
      "  (1, 22)\t0.2635440111190765\n",
      "  (1, 18)\t0.2635440111190765\n",
      "  (1, 15)\t0.2635440111190765\n",
      "  (1, 13)\t0.2635440111190765\n",
      "  (1, 12)\t0.2635440111190765\n",
      "  (1, 9)\t0.2635440111190765\n",
      "  (1, 8)\t0.2635440111190765\n",
      "  (1, 6)\t0.2635440111190765\n",
      "  (1, 4)\t0.42525129752567803\n",
      "  (1, 3)\t0.2635440111190765\n",
      "  (2, 27)\t0.31752680284846835\n",
      "  (2, 19)\t0.4741246485558491\n",
      "  (2, 11)\t0.4741246485558491\n",
      "  (2, 10)\t0.4741246485558491\n",
      "  (2, 5)\t0.4741246485558491\n",
      "  (3, 25)\t0.38775666010579296\n",
      "  (3, 23)\t0.38775666010579296\n",
      "  (3, 21)\t0.38775666010579296\n",
      "  (3, 20)\t0.38775666010579296\n",
      "  (3, 17)\t0.38775666010579296\n",
      "  (3, 1)\t0.38775666010579296\n",
      "  (3, 0)\t0.3128396318588854\n",
      "  (4, 26)\t0.2959842226518677\n",
      "  (4, 0)\t0.9551928286692534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "    \"hello, how are you?\",\n",
    "    \"im getting bored at home. And you? What do you think?\",\n",
    "    \"did you know about counts\",\n",
    "    \"let's see if this works!\",\n",
    "    \"YES!!!!\"\n",
    "]\n",
    "\n",
    "# initialize TfidfVectorizer with word_tokenize from nltk\n",
    "# as the tokenizer\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "# fit the vectorizer on corpus\n",
    "tfv.fit(corpus)\n",
    "\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ea853",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12e172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hi', ',', 'how'), (',', 'how', 'are'), ('how', 'are', 'you'), ('are', 'you', '?')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# let's see 3 grams\n",
    "N = 3\n",
    "# input sentence\n",
    "sentence = \"hi, how are you?\"\n",
    "# tokenized sentence\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "# generate n_grams\n",
    "n_grams = list(ngrams(tokenized_sentence, N))\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c60812",
   "metadata": {},
   "source": [
    "## stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a67826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nikolay/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c8f9fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=fishing\n",
      "stemmed_word=fish\n",
      "lemma=fishing\n",
      "\n",
      "word=fishes\n",
      "stemmed_word=fish\n",
      "lemma=fish\n",
      "\n",
      "word=fished\n",
      "stemmed_word=fish\n",
      "lemma=fished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "words = [\"fishing\", \"fishes\", \"fished\"]\n",
    "\n",
    "for word in words:\n",
    "    print(f\"word={word}\")\n",
    "    print(f\"stemmed_word={stemmer.stem(word)}\")\n",
    "    print(f\"lemma={lemmatizer.lemmatize(word)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a521e2",
   "metadata": {},
   "source": [
    "## topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6aee1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(s):\n",
    "    \"\"\"\n",
    "    This function cleans the text a bit\n",
    "    :param s: string\n",
    "    :return: cleaned string\n",
    "    \"\"\"\n",
    "    # split by all whitespaces\n",
    "    s = s.split()\n",
    "    \n",
    "    # join tokens by single space\n",
    "    # why we do this?\n",
    "    # this will remove all kinds of weird space\n",
    "    # \"hi.   how are you\" becomes\n",
    "    # \"hi. how are you\"\n",
    "    s = \" \".join(s)\n",
    "    \n",
    "    # remove all punctuations using regex and string module\n",
    "    s = re.sub(f'[{re.escape(string.punctuation)}]', '', s)\n",
    "    \n",
    "    # you can add more cleaning here if you want\n",
    "    # and then return the cleaned string\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6254b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a corpus of sentences\n",
    "# we read only 10k samples from training data\n",
    "# for this example\n",
    "corpus = pd.read_csv(\"../input/imdb.csv\", nrows=10000)\n",
    "corpus.loc[:, \"review\"] = corpus.review.apply(clean_text)\n",
    "corpus = corpus.review.values\n",
    "\n",
    "# initialize TfidfVectorizer with word_tokenize from nltk\n",
    "# as the tokenizer\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "# fit the vectorizer on corpus\n",
    "tfv.fit(corpus)\n",
    "\n",
    "# transform the corpus using tfidf\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "\n",
    "# initialize SVD with 10 components\n",
    "svd = decomposition.TruncatedSVD(n_components=10)\n",
    "\n",
    "# fit SVD\n",
    "corpus_svd = svd.fit(corpus_transformed)\n",
    "\n",
    "# choose first sample and create a dictionary\n",
    "# of feature names and their scores from svd\n",
    "# you can change the sample_index variable to\n",
    "# get dictionary for any other sample\n",
    "sample_index = 0\n",
    "feature_scores = dict(\n",
    "    zip(\n",
    "        tfv.get_feature_names(),\n",
    "        corpus_svd.components_[sample_index]\n",
    "    )\n",
    ")\n",
    "\n",
    "# once we have the dictionary, we can now\n",
    "# sort it in decreasing order and get the\n",
    "# top N topics\n",
    "N = 5\n",
    "print(sorted(feature_scores, key=feature_scores.get, reverse=True)[:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "for sample_index in range(5):\n",
    "    feature_scores = dict(\n",
    "        zip(\n",
    "            tfv.get_feature_names(),\n",
    "            corpus_svd.components_[sample_index]\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        sorted(\n",
    "            feature_scores,\n",
    "            key=feature_scores.get,\n",
    "            reverse=True\n",
    "        )[:N]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bece1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(word_index, embedding_file, vector_length=300):\n",
    "    \"\"\"\n",
    "    A general function to create embedding matrix\n",
    "    :param word_index: word:index dictionary\n",
    "    :param embedding_file: path to embeddings file\n",
    "    :param vector_length: length of vector\n",
    "    \"\"\"\n",
    "    max_features = len(word_index) + 1\n",
    "    words_to_find = list(word_index.keys())\n",
    "    more_words_to_find = []\n",
    "    \n",
    "    for wtf in words_to_find:\n",
    "        more_words_to_find.append(wtf)\n",
    "        more_words_to_find.append(str(wtf).capitalize())\n",
    "    more_words_to_find = set(more_words_to_find)\n",
    "    \n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(\n",
    "        get_coefs(*o.strip().split(\" \"))\n",
    "        for o in open(embedding_file)\n",
    "        if o.split(\" \")[0]\n",
    "        in more_words_to_find\n",
    "        and len(o) > 100\n",
    "    )\n",
    "    \n",
    "    embedding_matrix = np.zeros((max_features, vector_length))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(\n",
    "                str(word).capitalize()\n",
    "            )\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(\n",
    "                str(word).upper()\n",
    "            )\n",
    "        if (embedding_vector is not None\n",
    "            and len(embedding_vector) == vector_length):\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173b1de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
